{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment2_nlp.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpcLLE1JxI-3"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XaGnxOkyOlr",
        "outputId": "d11eb0cc-b14c-4b06-dcad-3687538558bf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "55nBFrz9yPgr",
        "outputId": "2c2839f1-db7b-4d44-bb03-3f554f492630"
      },
      "source": [
        "def get_labels_and_count():\n",
        "  count=0\n",
        "  d_rels={}\n",
        "  with open(\"/content/drive/MyDrive/nlp_assignments/assignment2/aassignment2/dataset/nyt29/relations.txt\") as rels:\n",
        "    for line in rels:\n",
        "      d_rels[line.strip('\\n')]=torch.tensor(count)\n",
        "      count+=1\n",
        "    d_rels['none']=torch.tensor(count)\n",
        "    count+=1\n",
        "    for key in d_rels:\n",
        "      d_rels[key]=F.one_hot(d_rels[key], num_classes=count)\n",
        "    return d_rels,count\n",
        "\n",
        "def get_entities_and_anns_wo_manipulation(annotation):\n",
        "  annos=annotation.split('|')\n",
        "  indices=[]\n",
        "  entities=[]\n",
        "  for ann in annos:\n",
        "    a=ann.split()\n",
        "    indice=[]\n",
        "    entity=[]\n",
        "    count=0\n",
        "    for item in a:\n",
        "      if item.isnumeric():\n",
        "        count+=1\n",
        "        entity.append(int(item))\n",
        "        if count==2:\n",
        "          count=0\n",
        "          indice.append(entity)\n",
        "          if not entity in entities:\n",
        "            entities.append(entity)\n",
        "          entity=[]\n",
        "      elif item in d_rels:\n",
        "        num=d_rels[item]\n",
        "        indice.append(num)\n",
        "      else:\n",
        "        print(\"ERROR: UNEXPECTED ENTRY FOUND IN TUPLE FILE\")\n",
        "    indices.append(indice)\n",
        "  datum=[]\n",
        "  for i in entities:\n",
        "    for j in entities:\n",
        "      if i==j:\n",
        "        continue;\n",
        "      data=[]\n",
        "      data.append(i)\n",
        "      data.append(j)\n",
        "      res=torch.zeros(l_count)\n",
        "      for src,dest,label in indices:\n",
        "        if (i==src and j==dest):\n",
        "          res = (res+label).float()\n",
        "      if (torch.sum(res)==torch.tensor(0.)): \n",
        "        res=d_rels['none'].float()\n",
        "      data.append(res)\n",
        "      datum.append(data)\n",
        "  return datum\n",
        "\n",
        "\n",
        "\n",
        "d_rels,l_count=get_labels_and_count()\n",
        "sentences=[]\n",
        "with open('/content/drive/MyDrive/nlp_assignments/assignment2/aassignment2/dataset/nyt29/train.sent') as s_file:\n",
        "  sentences=s_file.read().splitlines()\n",
        "with open('/content/drive/MyDrive/nlp_assignments/assignment2/aassignment2/dataset/nyt29/train.pointer') as l_file:\n",
        "  anns=l_file.read().splitlines()\n",
        "dataset=[]\n",
        "for sentence,annotation in zip(sentences,anns):\n",
        "  datum=get_entities_and_anns_wo_manipulation(annotation)\n",
        "  for data in datum:\n",
        "    data.append(sentence)\n",
        "    dataset.append(data)\n",
        "train_df=pd.DataFrame(dataset, columns=['source_entity','destination_entity','labels','text'])\n",
        "print(len(dataset))\n",
        "print(train_df['labels'].size)\n",
        "train_df[0:5]\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "146588\n",
            "146588\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source_entity</th>\n",
              "      <th>destination_entity</th>\n",
              "      <th>labels</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[12, 12]</td>\n",
              "      <td>[9, 9]</td>\n",
              "      <td>[tensor(1.), tensor(0.), tensor(0.), tensor(0....</td>\n",
              "      <td>then terrorism struck again , this time in the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[9, 9]</td>\n",
              "      <td>[12, 12]</td>\n",
              "      <td>[tensor(0.), tensor(1.), tensor(1.), tensor(0....</td>\n",
              "      <td>then terrorism struck again , this time in the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[16, 17]</td>\n",
              "      <td>[32, 32]</td>\n",
              "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(1....</td>\n",
              "      <td>a12 new york\\/region b1-7 enclave for middle c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[16, 17]</td>\n",
              "      <td>[19, 21]</td>\n",
              "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
              "      <td>a12 new york\\/region b1-7 enclave for middle c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[32, 32]</td>\n",
              "      <td>[16, 17]</td>\n",
              "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
              "      <td>a12 new york\\/region b1-7 enclave for middle c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  source_entity  ...                                               text\n",
              "0      [12, 12]  ...  then terrorism struck again , this time in the...\n",
              "1        [9, 9]  ...  then terrorism struck again , this time in the...\n",
              "2      [16, 17]  ...  a12 new york\\/region b1-7 enclave for middle c...\n",
              "3      [16, 17]  ...  a12 new york\\/region b1-7 enclave for middle c...\n",
              "4      [32, 32]  ...  a12 new york\\/region b1-7 enclave for middle c...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "1h4JSpvaI5oE",
        "outputId": "fb79e00f-cb9e-443b-bc1b-bd95cc431650"
      },
      "source": [
        "\n",
        "sentences=[]\n",
        "with open('/content/drive/MyDrive/nlp_assignments/assignment2/aassignment2/dataset/nyt29/dev.sent') as s_file:\n",
        "  sentences=s_file.read().splitlines()\n",
        "with open('/content/drive/MyDrive/nlp_assignments/assignment2/aassignment2/dataset/nyt29/dev.pointer') as l_file:\n",
        "  anns=l_file.read().splitlines()\n",
        "dataset=[]\n",
        "for sentence,annotation in zip(sentences,anns):\n",
        "  datum=get_entities_and_anns_wo_manipulation(annotation)\n",
        "  for data in datum:\n",
        "    data.append(sentence)\n",
        "    dataset.append(data)\n",
        "val_df=pd.DataFrame(dataset, columns=['source_entity','destination_entity','labels','text'])\n",
        "print(len(dataset))\n",
        "print(val_df['labels'].size)\n",
        "val_df[0:5]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16344\n",
            "16344\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source_entity</th>\n",
              "      <th>destination_entity</th>\n",
              "      <th>labels</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[50, 51]</td>\n",
              "      <td>[17, 17]</td>\n",
              "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
              "      <td>mr. scruggs -- who is arguing the case with hi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[50, 51]</td>\n",
              "      <td>[69, 70]</td>\n",
              "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
              "      <td>mr. scruggs -- who is arguing the case with hi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[50, 51]</td>\n",
              "      <td>[72, 74]</td>\n",
              "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
              "      <td>mr. scruggs -- who is arguing the case with hi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[50, 51]</td>\n",
              "      <td>[62, 62]</td>\n",
              "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
              "      <td>mr. scruggs -- who is arguing the case with hi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[17, 17]</td>\n",
              "      <td>[50, 51]</td>\n",
              "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
              "      <td>mr. scruggs -- who is arguing the case with hi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  source_entity  ...                                               text\n",
              "0      [50, 51]  ...  mr. scruggs -- who is arguing the case with hi...\n",
              "1      [50, 51]  ...  mr. scruggs -- who is arguing the case with hi...\n",
              "2      [50, 51]  ...  mr. scruggs -- who is arguing the case with hi...\n",
              "3      [50, 51]  ...  mr. scruggs -- who is arguing the case with hi...\n",
              "4      [17, 17]  ...  mr. scruggs -- who is arguing the case with hi...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HCUNTpUz9JO",
        "outputId": "472dcc81-3883-40ad-f754-88f0a6c35dec"
      },
      "source": [
        "! pip install tokenizers\n",
        "! pip install transformers"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.10.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.11.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.19)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIuZNdQLDiTn",
        "outputId": "177f0e66-41ff-40f0-89c3-e69bda14a813"
      },
      "source": [
        "from transformers import BertTokenizer, BertModel,DistilBertTokenizer,DistilBertModel\n",
        "encoder = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n",
        "special_tokens_dict = { 'additional_special_tokens':['SSE','SEE','DSE','DEE']}\n",
        "num_added_toks = encoder.add_special_tokens(special_tokens_dict)\n",
        "text=train_df['text'][0]\n",
        "src_entity=train_df['source_entity'][0]\n",
        "dst_entity=train_df['destination_entity'][0]\n",
        "splt=text.split(' ')\n",
        "print(text,src_entity,dst_entity)\n",
        "if (src_entity[1]<dst_entity[0]):\n",
        "  new_text=splt[0:src_entity[0]] + ['SSE'] + splt[src_entity[0]:src_entity[1]+1] + ['SEE'] + splt[src_entity[1]+1:dst_entity[0]] + ['DSE'] + splt[dst_entity[0]:dst_entity[1]+1] + ['DEE'] + splt[dst_entity[1]+1:]\n",
        "else:\n",
        "  new_text=splt[0:dst_entity[0]] + ['DSE'] + splt[dst_entity[0]:dst_entity[1]+1] + ['DEE'] + splt[dst_entity[1]+1:src_entity[0]] + ['SSE'] + splt[src_entity[0]:src_entity[1]+1] + ['SEE'] + splt[src_entity[1]+1:]\n",
        "print(new_text)\n",
        "output=encoder.encode_plus(new_text, add_special_tokens=True, padding='max_length', max_length=25,truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "print(output.tokens)\n",
        "# train_df[0:5]\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "then terrorism struck again , this time in the indonesia capital of jakarta . [12, 12] [9, 9]\n",
            "['then', 'terrorism', 'struck', 'again', ',', 'this', 'time', 'in', 'the', 'DSE', 'indonesia', 'DEE', 'capital', 'of', 'SSE', 'jakarta', 'SEE', '.']\n",
            "<bound method BatchEncoding.tokens of {'input_ids': tensor([[  101,  1173, 12010,  4168,  1254,   117,  1142,  1159,  1107,  1103,\n",
            "         28998,   100, 28999,  2364,  1104, 28996,   100, 28997,   119,   102,\n",
            "             0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
            "         0]])}>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Fvqha0iyxgY"
      },
      "source": [
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "class nyt29dataset(Dataset):\n",
        "  def __init__(self, df, encoder):\n",
        "    self.comb_dat=df\n",
        "    self.text=df['text']\n",
        "    self.labels=df['labels']\n",
        "    self.src_entity=df['source_entity']\n",
        "    self.dst_entity=df['destination_entity']\n",
        "    self.encoder=encoder\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.labels.size\n",
        "\n",
        "# will have to add special token so that the entity id is not lost\n",
        "  def __getitem__(self,item):\n",
        "    text=self.text[item]\n",
        "    label=self.labels[item]\n",
        "    src_ent=self.src_entity[item]\n",
        "    dst_ent=self.dst_entity[item]\n",
        "    splt=text.split(' ')\n",
        "    if (src_ent[1]<dst_ent[0]):\n",
        "      new_text=splt[0:src_ent[0]] + ['SSE'] + splt[src_ent[0]:src_ent[1]+1] + ['SEE'] + splt[src_ent[1]+1:dst_ent[0]] + ['DSE'] + splt[dst_ent[0]:dst_ent[1]+1] + ['DEE'] + splt[dst_ent[1]+1:]\n",
        "    else:\n",
        "      new_text=splt[0:dst_ent[0]] + ['DSE'] + splt[dst_ent[0]:dst_ent[1]+1] + ['DEE'] + splt[dst_ent[1]+1:src_ent[0]] + ['SSE'] + splt[src_ent[0]:src_ent[1]+1] + ['SEE'] + splt[src_ent[1]+1:]    \n",
        "    encoding=encoder.encode_plus(new_text, add_special_tokens=True, padding='max_length', max_length=512,truncation=True, return_tensors=\"pt\")\n",
        "    sse=(encoding['input_ids'] == 28996).nonzero()[0][1]\n",
        "    see=(encoding['input_ids'] == 28997).nonzero()[0][1]\n",
        "    dse=(encoding['input_ids'] == 28998).nonzero()[0][1]\n",
        "    dee=(encoding['input_ids'] == 28999).nonzero()[0][1]\n",
        "    # a=np.full((512),-1)\n",
        "    # b=np.full((512),-1)\n",
        "    # for i,pos in enumerate(range(sse+1,see)):\n",
        "    #   a[i]=pos\n",
        "    # for i,pos in enumerate(range(dse+1,dee)):\n",
        "    #   b[i]=pos\n",
        "    # b=list(range(dse+1,dee))\n",
        "    # print(sse,see,dse,dee,a,b)\n",
        "    # print(encoding['input_ids'].shape,encoding['attention_mask'].shape,label.shape)\n",
        "    # return encoding['input_ids'].squeeze(0), encoding['attention_mask'].squeeze(0),label,torch.tensor([sse+1,see]),torch.tensor([dse+1,dee])\n",
        "    return encoding['input_ids'].squeeze(0), encoding['attention_mask'].squeeze(0),label,sse,see,dse,dee\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjcAwViD_3EA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dd8de8a-de63-4682-c9b1-76a5dd3d19b9"
      },
      "source": [
        "train_dataset=nyt29dataset(train_df, encoder)\n",
        "train_dataloader=DataLoader(train_dataset, batch_size=8)\n",
        "val_dataset=nyt29dataset(val_df, encoder)\n",
        "val_dataloader=DataLoader(val_dataset, batch_size=8)\n",
        "inputids, attentionmasks, label, sse, see,dse,dee=next(iter(train_dataloader))\n",
        "print(sse,see,dse,dee)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([15, 10, 17, 17, 35, 35, 22, 20]) tensor([17, 12, 20, 20, 37, 37, 26, 24]) tensor([10, 15, 35, 22, 17, 20, 17, 35]) tensor([12, 17, 37, 26, 20, 24, 20, 37])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0loZt6fdCKQA",
        "outputId": "efa28232-9140-468f-cf66-69d165def841"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "class ml_relation_classifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    model = DistilBertModel.from_pretrained('distilbert-base-cased')\n",
        "    model.resize_token_embeddings(len(encoder)) \n",
        "    super(ml_relation_classifier,self).__init__()\n",
        "    self.DBert = model\n",
        "    self.classifier = nn.Sequential(\n",
        "          nn.Dropout(0.1),\n",
        "          nn.Linear(3840, 30),\n",
        "        )\n",
        "  def batch_op_get_maxpooled_ent(se, ee, hidden_state):\n",
        "    max_ent=torch.empty((8,768), dtype=torch.float)\n",
        "    iter=0\n",
        "    for i,j in zip(se+1,ee):\n",
        "      a=list(range(i,j))\n",
        "      # print(hidden_state[0][iter,a,:].shape)\n",
        "      entity,indices=torch.max(hidden_state[0][iter,a,:],dim=0)\n",
        "      max_ent[iter,:]=entity\n",
        "      iter+=1\n",
        "  def forward(self,input_ids,attention_masks,sse,see,dse,dee):\n",
        "    # print(dst_ent_span)\n",
        "    # for dst in range(dst_ent_span[:][0],dst_ent_span[:][1]):\n",
        "    #   print(dst)\n",
        "    # print(dst_ent=dst_ent_span if (dst_ent_span>=0))\n",
        "    # print((dst_ent_span[:]>=0).nonzero(as_tuple=True))\n",
        "    hidden_state=self.DBert(input_ids,attention_masks)\n",
        "    # print(hidden_state[0][:,dst_ent_span,:].shape)\n",
        "    # print(sse.shape)\n",
        "    cls=hidden_state[0][:,0,:]\n",
        "    sse=hidden_state[0][range(0,8),sse,:]\n",
        "    see=hidden_state[0][range(0,8),see,:]\n",
        "    dse=hidden_state[0][range(0,8),dse,:]\n",
        "    dee=hidden_state[0][range(0,8),dee,:]\n",
        "    # print(cls.shape,sse.shape)\n",
        "    input=torch.cat((cls, sse, see,dse,dee), dim=1)\n",
        "    # iter=0\n",
        "    # for i,j in zip(sse+1,see):\n",
        "    #   a=list(range(i,j))\n",
        "    #   # print(hidden_state[0][iter,a,:].shape)\n",
        "    #   entity,indices=torch.max(hidden_state[0][:,a,:],dim=0)\n",
        "    #   mp_src_ent[iter,:]=entity\n",
        "    #   iter+=1\n",
        "    # iter=0\n",
        "    # for i,j in zip(dse+1,dee):\n",
        "    #   a=list(range(i,j))\n",
        "    #   # print(hidden_state[0][iter,a,:].shape)\n",
        "    #   entity,indices=torch.max(hidden_state[0][iter,a,:],dim=0)\n",
        "    #   mp_src_ent[iter,:]=entity\n",
        "    #   iter+=1\n",
        "    # print(cls,mp_src_ent,mp_dst_ent)\n",
        "    # print(cls.shape,mp_src_ent.shape,mp_dst_ent.shape)\n",
        "    # mp_src_ent = mp_src_ent.to(device)\n",
        "    # mp_dst_ent = mp_dst_ent.to(device)\n",
        "    # print(cls.get_device(),mp_src_ent.get_device(),mp_dst_ent.get_device())\n",
        "    # input=torch.cat((cls, mp_src_ent, mp_dst_ent), dim=1)\n",
        "    # print('input')\n",
        "    # print(input)\n",
        "    # input.to(device)\n",
        "    # print(input.shape)\n",
        "    output=self.classifier(input)\n",
        "    # print('output')\n",
        "    # print(output)\n",
        "    # logits=torch.sigmoid(output)\n",
        "    return output\n",
        "    # return logits\n",
        "rel_ex=ml_relation_classifier()\n",
        "rel_ex(inputids,attentionmasks,sse,see,dse,dee)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 6.7844e-01, -2.0489e-01,  9.2212e-02,  1.4837e-01, -1.6900e-01,\n",
              "         -1.2007e-01,  3.9719e-01,  4.5350e-01,  7.5255e-02, -4.6530e-02,\n",
              "         -2.3681e-02,  4.8042e-01, -2.4766e-01,  1.9262e-01,  5.0460e-01,\n",
              "         -3.8832e-03, -9.4717e-02,  1.9432e-01, -2.7174e-01,  1.5057e-02,\n",
              "         -4.2471e-02,  2.0808e-01, -5.7133e-01,  6.3727e-02,  2.2915e-01,\n",
              "          3.2209e-01, -3.2095e-02,  2.5103e-01, -2.1716e-01,  2.4159e-01],\n",
              "        [ 4.8893e-01, -2.7876e-01, -8.1300e-02,  1.1518e-01, -1.2056e-01,\n",
              "         -8.5771e-02,  3.0293e-01,  4.9107e-01,  1.9307e-01, -1.1917e-01,\n",
              "         -1.3764e-01,  4.0252e-01, -2.3123e-01,  1.9459e-01,  4.3851e-01,\n",
              "         -2.9853e-02, -2.6557e-01,  8.9576e-02, -2.4030e-01,  1.1014e-01,\n",
              "          4.0426e-02,  2.4999e-01, -4.3879e-01,  6.3840e-02,  1.8220e-01,\n",
              "          3.8650e-01,  4.3892e-02,  1.8680e-01, -1.0683e-01,  1.5152e-01],\n",
              "        [ 4.7288e-01,  3.7371e-02,  4.5432e-02, -2.2632e-01, -1.5324e-01,\n",
              "         -3.2251e-01,  5.9737e-01,  2.7046e-01,  1.9903e-01,  4.7093e-02,\n",
              "         -2.7963e-01,  3.6792e-01, -1.9311e-01,  1.8248e-01,  2.7098e-01,\n",
              "         -3.4019e-01, -1.3963e-01, -5.4660e-02, -1.9847e-01,  3.5486e-01,\n",
              "         -5.6590e-02,  2.4425e-01, -4.0028e-01, -1.9314e-01,  2.2101e-01,\n",
              "          2.4030e-01, -2.0931e-01,  1.9619e-01, -4.7538e-01,  3.6508e-02],\n",
              "        [ 6.2051e-01,  8.2295e-03,  3.0932e-02, -1.4148e-01, -1.6763e-01,\n",
              "         -5.2609e-04,  5.0238e-01,  3.4542e-01,  9.0956e-03, -9.8896e-02,\n",
              "         -3.6032e-01,  2.8086e-01, -1.0870e-01,  1.0401e-01,  2.4899e-01,\n",
              "         -1.0293e-01, -2.4127e-01, -1.8565e-02, -1.1312e-01,  2.8700e-01,\n",
              "         -2.5679e-01,  3.0384e-01, -4.7373e-01, -1.3617e-01,  1.3117e-01,\n",
              "          3.1780e-01, -1.3300e-01,  6.8518e-02, -4.1601e-01, -1.8454e-02],\n",
              "        [ 5.0718e-01, -1.5874e-01,  1.3353e-01, -9.0144e-02, -1.2614e-01,\n",
              "         -1.3649e-01,  3.7440e-01,  3.6506e-01,  1.1493e-01, -9.4387e-03,\n",
              "         -7.1437e-02,  5.3041e-01, -3.9950e-01,  3.1726e-01,  5.1460e-01,\n",
              "         -1.0085e-01, -1.1373e-01,  1.0030e-02, -4.7509e-01,  1.9926e-01,\n",
              "         -9.3474e-02,  1.5220e-01, -5.6954e-01, -1.5741e-01, -8.3099e-02,\n",
              "          3.8027e-01,  1.5886e-02,  1.2582e-01, -2.5291e-01,  4.4009e-02],\n",
              "        [ 4.9665e-01,  3.1189e-02,  5.2372e-02, -1.7494e-02, -2.0027e-01,\n",
              "         -3.3070e-02,  3.5347e-01,  3.2306e-01,  1.1386e-01, -1.0280e-02,\n",
              "         -1.7960e-01,  4.7237e-01, -3.6708e-01,  2.6091e-01,  4.9722e-01,\n",
              "         -3.1255e-02, -1.7521e-01,  3.5257e-02, -2.9148e-01,  1.8804e-01,\n",
              "         -7.2913e-02,  3.1819e-01, -5.2786e-01,  1.1429e-01, -7.5806e-02,\n",
              "          4.5894e-01, -7.0977e-02,  6.6720e-02, -1.6046e-01,  8.9564e-02],\n",
              "        [ 4.3998e-01, -2.8135e-02, -1.9532e-02, -5.8708e-02, -2.5216e-01,\n",
              "         -1.2494e-01,  4.2881e-01,  3.2505e-01,  1.3160e-01, -1.0913e-01,\n",
              "         -3.6323e-02,  3.3282e-01, -3.3924e-01,  1.9999e-01,  4.7886e-01,\n",
              "         -4.3919e-02, -8.1423e-02, -3.7756e-02, -2.5751e-01,  3.8291e-02,\n",
              "         -5.3984e-02,  4.3093e-01, -4.5901e-01, -1.0559e-01, -3.5728e-02,\n",
              "          4.3045e-01, -5.7072e-03,  3.0946e-01, -3.6053e-02, -1.4866e-02],\n",
              "        [ 4.6771e-01, -2.2877e-01,  1.3597e-01, -6.6907e-02, -2.1200e-01,\n",
              "         -8.6222e-02,  4.1497e-01,  3.8640e-01, -1.7646e-02,  6.4022e-02,\n",
              "         -9.7216e-02,  3.6498e-01, -3.8691e-01,  2.8248e-01,  6.5926e-01,\n",
              "         -1.5807e-01, -1.2410e-01,  4.5175e-02, -3.4917e-01,  2.6118e-01,\n",
              "          9.2154e-02,  1.2491e-01, -4.6862e-01, -6.4383e-02,  2.0651e-01,\n",
              "          4.8874e-01, -3.8736e-02,  2.6335e-01, -1.9874e-01,  1.9545e-01]],\n",
              "       grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7zm7WDnrnGg"
      },
      "source": [
        "from transformers import AdamW\n",
        "from transformers import get_scheduler\n",
        "def init_model():\n",
        "  classifier=ml_relation_classifier()\n",
        "  classifier.to(device)\n",
        "  optimizer = AdamW(classifier.parameters(), lr=5e-5)\n",
        "  num_epochs = 1\n",
        "  num_training_steps = num_epochs * len(train_dataloader)\n",
        "  print(num_training_steps)\n",
        "  lr_scheduler = get_scheduler(\n",
        "      \"linear\",\n",
        "      optimizer=optimizer,\n",
        "      num_warmup_steps=0,\n",
        "      num_training_steps=num_training_steps\n",
        "  )\n",
        "  threshold=0.4\n",
        "  return classifier,optimizer,lr_scheduler,threshold,num_epochs\n",
        "\n",
        "loss_f=nn.BCEWithLogitsLoss()\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDg6wmEEMqz2"
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "def compute_metrics(preds,labels):\n",
        "  preds=preds.tolist()\n",
        "  labels=labels.tolist()\n",
        "  print(len(preds[3]),len(labels[5]))\n",
        "  for pred in preds: del pred[-1] \n",
        "  for label in labels: del label[-1] \n",
        "  print(len(preds[3]),len(labels[5]))\n",
        "  precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')\n",
        "  acc = accuracy_score(labels, preds)\n",
        "  print(\"Precision = \", precision)\n",
        "  print(\"recall = \", recall)\n",
        "  print(\"F1 score = \", f1)\n",
        "  print(\"accuracy = \", acc)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqgtJGucxL5_",
        "outputId": "a549629e-1d16-4e98-ed61-7e2332f51254"
      },
      "source": [
        "import itertools\n",
        "ml_rel_model,optimizer,lr_scheduler,threshold,num_epochs=init_model()\n",
        "ml_rel_model.train()\n",
        "for epoch in range(num_epochs):\n",
        "  rev_preds=[]\n",
        "  rev_labels=[]\n",
        "  iter=0\n",
        "  for input_ids,attention_masks,labels,sse, see,dse,dee in train_dataloader:\n",
        "    iter+=1\n",
        "    input_ids=input_ids.to(device)\n",
        "    attention_masks=attention_masks.to(device)\n",
        "    labels=labels.to(device)\n",
        "    ml_rel_model.zero_grad()\n",
        "    logits=ml_rel_model(input_ids,attention_masks,sse, see,dse,dee)\n",
        "    # print(logits)\n",
        "    # print(labels.float())\n",
        "    loss=loss_f(logits,labels.float())\n",
        "    if iter%99==0: \n",
        "      print(loss)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    lr_scheduler.step()\n",
        "  print(\"Terminated Epoch : \",epoch)\n",
        "  print(\"Saving model!!\")\n",
        "  torch.save(ml_rel_model,'/content/drive/MyDrive/nlp_assignments/assignment2/aassignment2/ml_rel_model_att_concat_spec_token.bin')\n",
        "  print(\"Proceed for evaluation\")\n",
        "  ml_rel_model.eval()\n",
        "  for input_ids,attention_masks,labels,sse, see,dse,dee in val_dataloader:\n",
        "    input_ids=input_ids.to(device)\n",
        "    attention_masks=attention_masks.to(device)\n",
        "    labels=labels.to(device)\n",
        "    with torch.no_grad():\n",
        "      logits=ml_rel_model(input_ids,attention_masks,sse, see,dse,dee)\n",
        "    logits=torch.sigmoid(logits)\n",
        "    predictions=torch.where(logits>threshold,1.,0.)\n",
        "    rev_preds.extend(predictions)\n",
        "    rev_labels.extend(labels)\n",
        "  compute_metrics(torch.stack(rev_preds).cpu(), torch.stack(rev_labels).cpu()) \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18324\n",
            "tensor(0.0796, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0785, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0634, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.1396, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0518, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0585, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0493, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0494, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0194, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0258, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0422, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0460, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0359, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0726, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0397, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0633, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0314, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0515, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0218, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.1279, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0393, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0432, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0648, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0142, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0200, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0952, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0478, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0237, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0634, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0344, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0796, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0824, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0088, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0210, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0640, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0648, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0239, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0536, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0584, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0186, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0967, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0362, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0063, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0276, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0571, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0410, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0461, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0282, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0385, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0159, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0413, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0754, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0314, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0388, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0314, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0272, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0560, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0343, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0704, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0586, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0470, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0912, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0232, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0167, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0245, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0439, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0303, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0681, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0281, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0485, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0263, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0316, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0291, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0686, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0332, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0381, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0679, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0291, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0457, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0399, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0359, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0257, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0107, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0255, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0967, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0248, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0274, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.1020, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0222, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0181, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0385, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0365, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0644, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0536, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0280, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0294, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0305, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0410, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0633, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0322, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0066, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0298, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0458, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0317, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0529, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0167, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0471, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0728, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0484, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0412, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0245, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0203, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0246, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0434, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0411, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0357, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0273, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0623, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0499, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.1098, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0158, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0377, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0040, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0478, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0112, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0125, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0170, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0609, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0913, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0535, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0271, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0562, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0774, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0068, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0841, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0520, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0673, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0392, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0680, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0362, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0270, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0140, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0250, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0238, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0469, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0030, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.1814, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0387, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0262, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0448, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0087, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0444, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0354, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0270, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0558, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0254, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0780, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0426, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0243, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "tensor(0.0243, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_7AK3ffWTFI"
      },
      "source": [
        "torch.save(ml_rel_model,'/content/drive/MyDrive/nlp_assignments/assignment2/aassignment2/ml_rel_model_att_concat_soec_tok2.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS3XHWRVc6DY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}